{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3\n",
    "This algorithm employs a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a decision tree.\n",
    "\n",
    "## Entropy : \n",
    "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogeneous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is equally divided then it has entropy of one.\n",
    "\n",
    "## Information Gain: \n",
    "The information gain is based on the decrease in entropy after a data-set is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).\n",
    "\n",
    "Step 1: Calculate entropy of the target.\n",
    "\n",
    "Step 2: The dataset is then split on the different attributes. The entropy for each branch is calculated. Then it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from the entropy before the split. The result is the Information Gain, or decrease in entropy.\n",
    "\n",
    "Step 3: Choose attribute with the largest information gain as the decision node, divide the dataset by its branches and repeat the same process on every branch.\n",
    "\n",
    "Step 4a: A branch with entropy of 0 is a leaf node.\n",
    "\n",
    "Step 4b: A branch with entropy more than 0 needs further splitting.\n",
    "\n",
    "Step 5: The ID3 algorithm is run recursively on the non-leaf branches, until all data is classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Index\n",
    "Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "\n",
    "It works with categorical target variable “Success” or “Failure”.\n",
    "It performs only Binary splits\n",
    "Higher the value of Gini higher the homogeneity.\n",
    "CART (Classification and Regression Tree) uses Gini method to create binary splits.\n",
    "\n",
    "## Steps to Calculate Gini for a split\n",
    "\n",
    "Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (p^2+q^2).\n",
    "Calculate Gini for split using weighted Gini score of each node of that split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Square\n",
    "It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable.\n",
    "\n",
    "It works with categorical target variable “Success” or “Failure”.\n",
    "It can perform two or more splits.\n",
    "Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n",
    "Chi-Square of each node is calculated using formula,\n",
    "Chi-square = ((Actual – Expected)^2 / Expected)^1/2\n",
    "It generates tree called CHAID (Chi-square Automatic Interaction Detector)\n",
    "\n",
    "## Steps to Calculate Chi-square for a split:\n",
    "\n",
    "Calculate Chi-square for individual node by calculating the deviation for Success and Failure both\n",
    "Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction in Variance\n",
    "\n",
    "Reduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population\n",
    "\n",
    "### Steps to calculate Variance:\n",
    "\n",
    "1- Calculate variance for each node.\n",
    "\n",
    "2- Calculate variance for each split as weighted average of each node variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1eb543ad2066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# model = tree.DecisionTreeRegressor() for regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Train the model using the training sets and check score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Predict Output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import tree\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create tree object \n",
    "model = tree.DecisionTreeClassifier() # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "# Train the model using the training sets and check score\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "#Predict Output\n",
    "predicted= model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
